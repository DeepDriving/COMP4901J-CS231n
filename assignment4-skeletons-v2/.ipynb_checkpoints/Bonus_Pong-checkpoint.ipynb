{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tackling Pong (bonus part)\n",
    "Basically borrow ideas from DQN\n",
    "\n",
    "### I implement a DQN to solve the pong game, as you can see at the last block, the rewards is indeed increaseing. But It need much time to finish it, and I have no more time to run it as it is already in study break. Hope this will win some bonus :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.misc\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the dependency setup is instructed in: https://github.com/openai/gym\n",
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(6)\n",
      "Box(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "# inspect the env\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "n_action = 6\n",
    "observation_size = 210 * 160 * 3\n",
    "\n",
    "def processState(states):\n",
    "    return np.reshape(states,[observation_size]) # 84 x 84 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experience buffer\n",
    "class experience_buffer():\n",
    "    def __init__(self, buffer_size = 50000):\n",
    "        self.buffer = []\n",
    "        self.buffer_size = buffer_size\n",
    "    \n",
    "    def add(self,experience):\n",
    "        if len(self.buffer) + len(experience) >= self.buffer_size:\n",
    "            self.buffer[0:(len(experience)+len(self.buffer))-self.buffer_size] = []\n",
    "        self.buffer.extend(experience)\n",
    "            \n",
    "    def sample(self,size):\n",
    "        return np.reshape(np.array( random.sample(self.buffer, size) ), [size,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updateTargetGraph(tfVars,tau):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign((var.value()*tau) + ((1-tau)*tfVars[idx+total_vars//2].value())))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Conv_13/Relu:0\", shape=(?, 3, 1, 64), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Qnetwork at 0x7f1db42adac8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Qnetwork():\n",
    "    def __init__(self,h_size):\n",
    "        #The network recieves a frame from the game, flattened into an array.\n",
    "        #It then resizes it and processes it through four convolutional layers.\n",
    "        #We use slim.conv2d to set up our network \n",
    "        self.scalarInput =  tf.placeholder(shape=[None, observation_size],dtype=tf.float32)\n",
    "        self.imageIn = tf.reshape(self.scalarInput,shape=[-1,210, 160, 3])\n",
    "        self.pool1 = tf.nn.max_pool(value = self.imageIn, ksize = [1, 4, 4, 1], strides = [1, 4, 4, 1], padding = 'VALID',)\n",
    "        self.conv1 = slim.conv2d( \\\n",
    "            inputs=self.pool1,num_outputs=32,kernel_size=[8,8],stride=[4,4],padding='VALID', biases_initializer=None)\n",
    "        self.conv2 = slim.conv2d( \\\n",
    "            inputs=self.conv1,num_outputs=64,kernel_size=[4,4],stride=[2,2],padding='VALID', biases_initializer=None)\n",
    "        self.conv3 = slim.conv2d( \\\n",
    "            inputs=self.conv2,num_outputs=64,kernel_size=[3,3],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "#         self.conv4 = slim.conv2d( \\\n",
    "#             inputs=self.conv3,num_outputs=h_size,kernel_size=[5,5],stride=[1,1],padding='VALID', biases_initializer=None)\n",
    "        # conv4 (N, 3, 1, h_size)\n",
    "#         print(self.conv3)\n",
    "        self.final_conv = self.conv3\n",
    "        self.final_conv_size = 3 * 1 * h_size\n",
    "        \n",
    "        # split the stream\n",
    "        self.splitA, self.splitV = tf.split(self.final_conv, 2, axis = 3)\n",
    "        # flatten the tensor\n",
    "        self.flattenA = tf.reshape(self.splitA, [-1, self.final_conv_size//2])\n",
    "        self.flattenV = tf.reshape(self.splitV, [-1, self.final_conv_size//2])\n",
    "        # affine layers\n",
    "        self.Advantage = tf.layers.dense(self.flattenA, n_action, \n",
    "                                         kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        self.Value = tf.layers.dense(self.flattenV, 1, \n",
    "                                     kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        #Then combine them together to get our final Q-values. \n",
    "        #Please refer to Equation (9) in [Dueling DQN](https://arxiv.org/pdf/1511.06581.pdf)\n",
    "        self.Qout = self.Value + tf.subtract(self.Advantage,tf.reduce_mean(self.Advantage,axis=1,keep_dims=True))\n",
    "        self.predict = tf.argmax(self.Qout,1)\n",
    "        \n",
    "        #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "        self.targetQ = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.actions = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        self.actions_onehot = tf.one_hot(self.actions, n_action, dtype=tf.float32)\n",
    "        \n",
    "        # The onehot eliminate the gradient. This mask out the Q of the actions not chonsen, and only keep the chosen one. \n",
    "        # This is also a way to prevent the update of the actions that are not chosen\n",
    "        self.Q = tf.reduce_sum(tf.multiply(self.Qout, self.actions_onehot), axis=1)\n",
    "        \n",
    "        self.loss = tf.reduce_mean(tf.square(self.targetQ - self.Q))\n",
    "        \n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate=0.0001)\n",
    "        self.updateModel = self.trainer.minimize(self.loss)\n",
    "Qnetwork(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "batch_size = 32 #How many experiences to use for each training step.\n",
    "update_freq = 4 #How often to perform a training step.\n",
    "y = .99 #Discount factor on the target Q-values\n",
    "startE = 1 #Starting chance of random action\n",
    "endE = 0.1 #Final chance of random action\n",
    "annealing_steps = 10000. #How many steps of training to reduce startE to endE.\n",
    "num_episodes = 5000 #How many episodes of game environment to train network with.\n",
    "pre_train_steps = 10000 #How many steps of random actions before training begins.\n",
    "max_epLength = 500 #The max allowed length of our episode.\n",
    "load_model = False #Whether to load a saved model.\n",
    "path = \"./pong_dqn\" #The path to save our model to.\n",
    "h_size = 64 #The size of the final convolutional layer before splitting it into Advantage and Value streams.\n",
    "tau = 0.001 #Rate to update target network toward primary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model\n",
      "Episode 9 reward: -15.7\n",
      "Episode 19 reward: -16.5\n",
      "Episode 29 reward: -16.6\n",
      "Episode 39 reward: -16.9\n",
      "Episode 49 reward: -18.1\n",
      "Episode 59 reward: -18.8\n",
      "Episode 69 reward: -19.2\n",
      "Episode 79 reward: -18.4\n",
      "Episode 89 reward: -19.9\n",
      "Episode 99 reward: -19.8\n",
      "Episode 109 reward: -18.7\n",
      "Episode 119 reward: -19.1\n",
      "Episode 129 reward: -19.4\n",
      "Episode 139 reward: -18.0\n",
      "Episode 149 reward: -15.9\n",
      "Episode 159 reward: -18.2\n",
      "Episode 169 reward: -15.2\n",
      "Episode 179 reward: -18.5\n",
      "Episode 189 reward: -19.0\n",
      "Episode 199 reward: -19.6\n",
      "Episode 209 reward: -19.8\n",
      "Episode 219 reward: -18.3\n",
      "Episode 229 reward: -17.8\n",
      "Episode 239 reward: -19.6\n",
      "Episode 249 reward: -17.8\n",
      "Episode 259 reward: -17.1\n",
      "Episode 269 reward: -17.4\n",
      "Episode 279 reward: -18.4\n",
      "Episode 289 reward: -16.0\n",
      "Episode 299 reward: -16.8\n",
      "Episode 309 reward: -15.9\n",
      "Episode 319 reward: -16.6\n",
      "Episode 329 reward: -17.5\n",
      "Episode 339 reward: -17.8\n",
      "Episode 349 reward: -16.0\n",
      "Episode 359 reward: -17.3\n",
      "Episode 369 reward: -14.8\n",
      "Episode 379 reward: -15.5\n",
      "Episode 389 reward: -15.3\n",
      "Episode 399 reward: -15.9\n",
      "Episode 409 reward: -13.6\n",
      "Episode 419 reward: -12.8\n",
      "Episode 429 reward: -15.5\n",
      "Episode 439 reward: -14.0\n",
      "Episode 449 reward: -11.3\n",
      "Episode 459 reward: -11.3\n",
      "Episode 469 reward: -13.3\n",
      "Episode 479 reward: -13.0\n",
      "Episode 489 reward: -12.8\n",
      "Episode 499 reward: -15.2\n",
      "Episode 509 reward: -15.3\n",
      "Episode 519 reward: -16.9\n",
      "Episode 529 reward: -16.4\n",
      "Episode 539 reward: -12.1\n",
      "Episode 549 reward: -13.7\n",
      "Episode 559 reward: -13.2\n",
      "Episode 569 reward: -13.7\n",
      "Episode 579 reward: -14.7\n",
      "Episode 589 reward: -13.4\n",
      "Episode 599 reward: -13.7\n",
      "Episode 609 reward: -16.0\n",
      "Episode 619 reward: -13.7\n",
      "Episode 629 reward: -10.1\n",
      "Episode 639 reward: -9.2\n",
      "Episode 649 reward: -11.5\n",
      "Episode 659 reward: -10.8\n",
      "Episode 669 reward: -14.4\n",
      "Episode 679 reward: -10.1\n",
      "Episode 689 reward: -11.3\n",
      "Episode 699 reward: -11.2\n",
      "Episode 709 reward: -10.7\n",
      "Episode 719 reward: -10.6\n",
      "Episode 729 reward: -9.2\n",
      "Episode 739 reward: -10.0\n",
      "Episode 749 reward: -7.8\n",
      "Episode 759 reward: -9.3\n",
      "Episode 769 reward: -10.5\n",
      "Episode 779 reward: -8.8\n",
      "Episode 789 reward: -7.7\n",
      "Episode 799 reward: -11.1\n",
      "Episode 809 reward: -10.0\n",
      "Episode 819 reward: -10.2\n",
      "Episode 829 reward: -10.6\n",
      "Episode"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "mainQN = Qnetwork(h_size)\n",
    "targetQN = Qnetwork(h_size)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetGraph(trainables,tau)\n",
    "\n",
    "myBuffer = experience_buffer()\n",
    "\n",
    "#Set the rate of random action decrease. \n",
    "e = startE\n",
    "stepDrop = (startE - endE)/annealing_steps\n",
    "\n",
    "#create lists to contain total rewards and steps per episode\n",
    "jList = []\n",
    "rList = []\n",
    "total_steps = 0\n",
    "\n",
    "#Make a path for our model to be saved in.\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    if load_model == True:\n",
    "        print('Loading Model...')\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    for i in range(num_episodes):\n",
    "        episodeBuffer = experience_buffer()\n",
    "        #Reset environment and get first new observation\n",
    "        s = env.reset()\n",
    "        s = processState(s)\n",
    "        d = False\n",
    "        rAll = 0\n",
    "        j = 0\n",
    "        #The Q-Network\n",
    "        while j < max_epLength: #If the agent takes longer than 50 moves to reach either of the blocks, end the trial.\n",
    "            j+=1\n",
    "            #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "            if np.random.rand(1) < e or total_steps < pre_train_steps:\n",
    "                a = np.random.randint(0,6)\n",
    "            else:\n",
    "                a = sess.run(mainQN.predict,feed_dict={mainQN.scalarInput:[s]})[0]\n",
    "            total_steps += 1\n",
    "            \n",
    "            # get new state. It do not return an \"info\"\n",
    "            s1, r, d, _ = env.step(a)\n",
    "            # resize the state\n",
    "            s1 = processState(s1)\n",
    "            # add the experience\n",
    "            # Note that it used exntend() method, so we need an additional dimension. \n",
    "            episodeBuffer.add( np.array([[s, a, r, s1, d]]) )\n",
    "            \n",
    "            if total_steps > pre_train_steps:\n",
    "                if e > endE:\n",
    "                    e -= stepDrop\n",
    "                \n",
    "                if total_steps % (update_freq) == 0:\n",
    "                    \n",
    "                    random_experience = myBuffer.sample(batch_size) # note that the it return a np.array\n",
    "                    # we use our primary network to chose an action (actionQ)\n",
    "                    # and our target network to generate the target Q-value for that action.\n",
    "                    # the input state is s1\n",
    "                    ''' Very wired behaviors here. Why 1d ndarray of 1d ndarray but not a 2d array'''\n",
    "#                     print(type(random_experience[:,3])) # numpy.ndarray\n",
    "#                     print(random_experience[0,3].shape) # (32,)\n",
    "                    \n",
    "#                     print(type(random_experience[:,3][0])) # numpy.ndarray\n",
    "#                     print(random_experience[:,3][0].shape) # (21168,)\n",
    "                    \n",
    "#                     random_experience = np.array(random_experience.tolist()) # not working\n",
    "\n",
    "                    actionQ = sess.run( mainQN.predict, feed_dict = {\n",
    "                            mainQN.scalarInput: np.vstack(random_experience[:,3])\n",
    "                        })\n",
    "                    predictQ = sess.run( targetQN.Qout, feed_dict = {\n",
    "                            targetQN.scalarInput: np.vstack(random_experience[:,3])\n",
    "                        })\n",
    "                    \n",
    "                    # get the Q value from the predictQ using actionQ (if take actions according to mainQN) \n",
    "                    # both are for the next state\n",
    "                    doubleQ = predictQ[range(batch_size), actionQ]\n",
    "                    # accoding to the equation\n",
    "                    targetQ = random_experience[:, 2] + y * doubleQ\n",
    "                    \n",
    "                    # update the mainQN\n",
    "                    _ = sess.run(mainQN.updateModel, feed_dict= {\n",
    "                            mainQN.scalarInput: np.vstack(random_experience[:,0]),\n",
    "                            mainQN.targetQ: targetQ,\n",
    "                            mainQN.actions: random_experience[:, 1]\n",
    "                        })\n",
    "                           \n",
    "                    updateTarget(targetOps,sess) #Update the target network toward the primary network.\n",
    "            rAll += r\n",
    "            s = s1\n",
    "            \n",
    "            if d == True:\n",
    "\n",
    "                break\n",
    "        \n",
    "        myBuffer.add(episodeBuffer.buffer)\n",
    "        jList.append(j)\n",
    "        rList.append(rAll)\n",
    "        #Periodically save the model. \n",
    "        if i % 1000 == 0:\n",
    "            saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "            print(\"Saved Model\")\n",
    "        if len(rList) % 10 == 0:\n",
    "            print(\"Episode\",i,\"reward:\",np.mean(rList[-10:]))\n",
    "    saver.save(sess,path+'/model-'+str(i)+'.ckpt')\n",
    "print(\"Mean reward per episode: \" + str(sum(rList)/num_episodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
